{
  "master": {
    "tasks": [
      {
        "id": 1,
        "title": "Initialize Project Structure and CI/CD Pipeline",
        "description": "Set up the foundational project structure, including the Git repository, Python environment, and basic dependency management. This task ensures a clean and scalable starting point for all subsequent development.",
        "details": "Create a new Git repository. Set up a Python 3.8+ virtual environment. Initialize a `requirements.txt` or `pyproject.toml` with core libraries like `pytest`. Create directory structure: `/shadow_ai` for source code, `/tests` for unit tests, and a `/docs` folder. Configure a basic CI pipeline (e.g., GitHub Actions) to run linters and tests on push.",
        "testStrategy": "Verify that the repository is cloneable, the virtual environment can be created, dependencies can be installed via `pip install`, and `pytest` runs successfully (even with no tests). The CI pipeline should trigger and pass on the initial commit.",
        "priority": "high",
        "dependencies": [],
        "status": "done",
        "subtasks": [
          {
            "id": 1,
            "title": "Initialize Git Repository and Directory Structure",
            "description": "Create the project's Git repository on GitHub and establish the standard directory layout for source code, tests, and documentation, including a Python-specific .gitignore file.",
            "dependencies": [],
            "details": "Create a new public repository on GitHub named 'shadow-ai'. Clone it locally. Create the following directories: `shadow_ai/`, `tests/`, and `docs/`. Add a placeholder `__init__.py` to `shadow_ai/` and `tests/` to make them recognizable as Python packages. Generate and commit a standard Python `.gitignore` file.\n<info added on 2025-08-04T09:55:54.604Z>\n**Completion Summary (Commit: 570ca7f):**\n- The repository and directory structure were successfully created.\n- The main package directory was named `shadow_ai/` (instead of `src/`) for better descriptiveness.\n- `__init__.py` files were added to `shadow_ai/` and `tests/`. The main `__init__.py` was populated with project metadata (`__version__`, `__author__`, `__email__`).\n- A comprehensive `.gitignore` file was generated, covering Python-specific artifacts, virtual environments, IDE files, and testing caches.\n- All changes were committed, marking the completion of the initial setup.\n</info added on 2025-08-04T09:55:54.604Z>",
            "status": "done",
            "testStrategy": "Verify the repository is cloneable from GitHub. Check that the `shadow_ai`, `tests`, `docs`, and `.gitignore` files/folders exist in the cloned repository."
          },
          {
            "id": 2,
            "title": "Set Up Conda Environment and pyproject.toml",
            "description": "Configure the project's isolated Python environment using Conda and initialize the `pyproject.toml` file for modern, PEP 621-compliant dependency management.",
            "dependencies": [],
            "details": "Create a Conda environment for the project using Python 3.10+ (e.g., `conda create -n shadow-ai python=3.10`). Activate the environment. Manually create or use a tool like Poetry/PDM to initialize a `pyproject.toml` file. Define project metadata (name, version, authors) and add `pytest` as a development dependency.\n<info added on 2025-08-04T10:11:05.386Z>\n**Implementation Notes:**\n- A `shadow-ai` conda environment was created with Python 3.11.13.\n- A comprehensive `pyproject.toml` was set up using modern PEP 621 standards, replacing the need for `setup.py` and `requirements.txt`.\n- Core dependencies were installed:\n  - **Production:** `fastapi`, `uvicorn[standard]`, `click`\n  - **Development:** `pytest`, `pytest-cov`, `ruff`, `mypy`, `httpx`\n- Initial configuration for `ruff` (for both linting and formatting) and `mypy` (for strict type checking) was added to `pyproject.toml`.\n- Environment setup documentation was created in `docs/environment.md`.\n- All installed tools were verified as operational (pytest 8.4.1, ruff 0.12.0, mypy 1.16.0).\n</info added on 2025-08-04T10:11:05.386Z>",
            "status": "done",
            "testStrategy": "Activate the conda environment. Run `pip install .[dev]` or equivalent. Verify that `pytest` is installed and its version can be checked (`pytest --version`)."
          },
          {
            "id": 3,
            "title": "Configure Code Linters and Formatters",
            "description": "Integrate and configure modern code quality tools within `pyproject.toml` to enforce a consistent coding style and catch common errors automatically.",
            "dependencies": [],
            "details": "Add `ruff` and `mypy` as development dependencies in `pyproject.toml`. Create configuration sections for these tools (e.g., `[tool.ruff]`, `[tool.mypy]`). Configure `ruff` to enforce Black-compatible formatting and a strict set of linting rules suitable for a new project. Configure `mypy` for strict type checking.\n<info added on 2025-08-04T10:17:58.387Z>\nConfiguration for `ruff` and `mypy` was successfully added to `pyproject.toml`.\n- **Ruff**: Configured for Black-compatible formatting (88-char line length) and a strict linting ruleset (E, F, I, W, B, C4, UP).\n- **Mypy**: Configured for strict type checking.\n\nA `.pre-commit-config.yaml` file was created to automate these checks on commit, including hooks for `ruff` (lint & format), `mypy`, and standard file cleanup.\n\nFor improved developer workflow on Windows, helper scripts were created:\n- `dev.bat`: A batch file providing simple access to commands (`lint`, `format`, `types`, `test`, `all`, `clean`).\n- `scripts/dev.ps1`: A PowerShell module with underlying helper functions.\n\nVerification was performed by running `ruff check`, `ruff format --check`, and `mypy .`, all of which passed successfully.\n</info added on 2025-08-04T10:17:58.387Z>",
            "status": "done",
            "testStrategy": "After installing dependencies, run `ruff check .`, `ruff format --check .`, and `mypy shadow_ai` from the command line. Verify they run without configuration errors."
          },
          {
            "id": 4,
            "title": "Create Initial Test Suite Placeholder",
            "description": "Establish the initial test suite by creating a simple placeholder test case. This ensures the testing framework is correctly configured and provides a target for the CI pipeline.",
            "dependencies": [],
            "details": "In the `/tests` directory, create a file named `test_initial_setup.py`. Inside this file, write a simple passing test function, such as `def test_project_initialization(): assert True`, to confirm that `pytest` can discover and execute tests.\n<info added on 2025-08-04T10:29:27.568Z>\nThe initial test suite was expanded significantly beyond the original scope. Instead of a single placeholder test, a comprehensive suite of 15 passing tests was created across three files: `test_initial_setup.py`, `test_cli.py`, and `test_detection_engine.py`. The setup tests verify Python version, directory structure, package metadata, configuration file existence, and dependency availability. The test run achieved 100% code coverage, with pytest configured for coverage reporting. All linting and type checks passed, verified via a `dev.bat` script.\n</info added on 2025-08-04T10:29:27.568Z>",
            "status": "done",
            "testStrategy": "Run `pytest` from the project's root directory. Verify that it discovers and runs the single test, reporting 1 passed test."
          },
          {
            "id": 5,
            "title": "Implement Basic GitHub Actions CI Workflow",
            "description": "Create a continuous integration workflow using GitHub Actions that automatically validates code quality and runs tests on every push and pull request.",
            "dependencies": [],
            "details": "Create a `.github/workflows/ci.yml` file. Define a workflow that triggers on `push` and `pull_request` to the main branch. The workflow should contain a single job with steps to: 1) Checkout code. 2) Set up Python 3.10. 3) Install project dependencies. 4) Run linter/formatter checks (`ruff check .` and `ruff format --check .`). 5) Run the test suite (`pytest`).\n<info added on 2025-08-04T10:40:52.141Z>\nThe implementation went significantly beyond the original scope, adding the following features:\n\nExpanded CI Workflow (ci.yml):\n- Multi-Python version testing matrix (3.9, 3.10, 3.11, 3.12).\n- Additional quality checks: mypy for type checking.\n- Security scanning: bandit and safety.\n- Test coverage reporting and integration with Codecov.\n- Triggers on push/pull_request to the 'develop' branch in addition to 'main'.\n\nNew Workflows:\n- An automated release workflow (release.yml) for GitHub Releases and PyPI publishing.\n- A Dependabot configuration (dependabot.yml) for automated dependency updates.\n\nTest Suite Expansion:\n- The test suite was built out to 15 passing tests, achieving 100% code coverage.\n\nDocumentation:\n- Updated README.md with CI/CD status badges and a development guide.\n- Added an MIT LICENSE file.\n</info added on 2025-08-04T10:40:52.141Z>",
            "status": "done",
            "testStrategy": "Push the initial project structure to the GitHub repository. Navigate to the 'Actions' tab on GitHub and verify that the CI workflow is triggered and passes successfully."
          }
        ]
      },
      {
        "id": 2,
        "title": "Develop Core Heuristic Detection Engine",
        "description": "Implement the primary AI detection logic based on the specified heuristic checks. This module will be the brain of the tool, responsible for analyzing code snippets and identifying patterns indicative of AI generation.",
        "details": "Create a Python module `engine.py`. Implement functions for: 1) Comment Pattern Analysis (regex for repetitive structures). 2) Variable Naming Analysis (check against a list of generic names like 'data', 'result', 'temp'). 3) Code Structure Analysis (use an AST parser like Python's `ast` module to check for overly consistent node structures). 4) AI Language Pattern matching in comments.",
        "testStrategy": "Create a suite of unit tests in `/tests` with sample code snippets. Include positive cases (known AI-generated code) and negative cases (known human-written code). Assert that specific heuristics are correctly triggered for each case.",
        "priority": "high",
        "dependencies": [
          1
        ],
        "status": "done",
        "subtasks": [
          {
            "id": 1,
            "title": "Implement Comment Pattern Analysis Heuristic",
            "description": "Develop the function to analyze comments in a code snippet for patterns indicative of AI generation, such as overly simplistic or repetitive comment structures.",
            "dependencies": [],
            "details": "In `engine.py`, create a function `analyze_comment_patterns(code_string)`. Use regular expressions to identify common, non-descriptive comment formats (e.g., `# [Function Name]`, `# [Variable Name]`) and check for high comment-to-code ratios. The function should return a raw metric, such as the count of matched patterns.\n<info added on 2025-08-04T11:02:29.680Z>\nThe `analyze_comment_patterns()` function has been implemented in `shadow_ai/engine.py`. It uses 14 regex patterns to detect generic AI-generated comments, calculates the comment-to-code ratio, and identifies repetitive comment structures via a helper function. The implementation is robust, handling edge cases like empty input. A comprehensive test suite in `tests/test_detection_engine.py` with 8 test cases was created, achieving 98% code coverage and validating functionality against various scenarios. The function returns a structured dictionary of metrics for integration with the confidence scoring module.\n</info added on 2025-08-04T11:02:29.680Z>",
            "status": "done",
            "testStrategy": "Create unit tests in `/tests/test_engine.py` with code snippets containing generic, repetitive comments and snippets with descriptive, human-like comments. Assert the correct pattern count is returned."
          },
          {
            "id": 2,
            "title": "Implement Variable Naming Analysis Heuristic",
            "description": "Create the logic to check for the use of overly generic variable and function names, which are common in AI-generated code examples.",
            "dependencies": [],
            "details": "In `engine.py`, create a function `analyze_variable_names(code_string)`. Define a configurable list of common generic names (e.g., 'data', 'result', 'temp', 'item'). Use the `ast` module to parse the code, extract all variable and function names, and calculate the percentage of generic names found.\n<info added on 2025-08-04T11:07:46.959Z>\nThe implementation now uses a comprehensive database of over 50 common AI-generated names, including generic function names ('process', 'handle') and single-letter variables ('i', 'j', 'k'). The AST parsing is more thorough, extracting class names and function arguments in addition to variables and functions, while also filtering out built-in names and keywords to focus on user-defined identifiers. The function's output is a structured dictionary containing the count of generic names, the total count of names, the calculated percentage, and a list of the specific generic names found. It gracefully handles syntax errors in the input code by returning a neutral result. The feature is validated by a suite of 8 comprehensive unit tests covering generic/descriptive names, mixed cases, and edge cases, all of which are passing.\n</info added on 2025-08-04T11:07:46.959Z>",
            "status": "done",
            "testStrategy": "Test with code that heavily uses generic names ('data', 'temp_var') and code that uses descriptive, domain-specific names. Assert the returned percentage is within an expected range for each case."
          },
          {
            "id": 3,
            "title": "Implement Code Structure Analysis using AST",
            "description": "Use Python's Abstract Syntax Tree (AST) module to analyze the structural consistency of the code, looking for overly uniform or simplistic structures often produced by AI.",
            "dependencies": [],
            "details": "In `engine.py`, create a function `analyze_code_structure(code_string)`. Use the `ast` module to parse the code. Traverse the tree to analyze metrics like function length, nesting depth, and the variety of node types used. The goal is to identify unnaturally consistent or simple structures.\n<info added on 2025-08-04T11:31:21.086Z>\nImplementation is complete and has been successfully tested. The `analyze_code_structure()` function was created in `shadow_ai/engine.py`. It uses AST parsing to analyze several key metrics: function length variance, average and maximum nesting depth, node type diversity, and control flow complexity. These are combined into a final 'structural uniformity score' from 0-100 to quantify suspicion.\n\nHelper functions `_calculate_max_nesting_depth()` and `_calculate_uniformity_score()` were also implemented. The feature is supported by 9 comprehensive test cases covering uniform/varied code, simple/complex patterns, and edge cases like syntax errors or empty input. All tests are passing. The function is robust, handles malformed code, and returns a detailed dictionary of metrics to be used by the confidence scoring module.\n</info added on 2025-08-04T11:31:21.086Z>",
            "status": "done",
            "testStrategy": "Provide code snippets with very simple, repetitive function structures and compare against more complex, varied human-written code. Ensure the function handles syntax errors gracefully by returning a neutral result."
          },
          {
            "id": 4,
            "title": "Implement AI Language Pattern Matching in Comments",
            "description": "Develop a function to scan comments and docstrings for specific phrases and conversational artifacts commonly left by AI code generators.",
            "dependencies": [],
            "details": "In `engine.py`, create a function `match_ai_language_patterns(code_string)`. Define a list of tell-tale AI phrases (e.g., 'As an AI language model', 'I cannot access real-time data', 'Here is an example'). Use case-insensitive regex to search for these patterns within comments and docstrings.\n<info added on 2025-08-04T11:45:30.350Z>\nImplementation is complete and has been comprehensively tested.\n\n**Key Features & Details:**\n- **Pattern Categories:** Expanded detection to cover 4 distinct categories: AI Self-References, Conversational Patterns, Disclaimer Patterns, and Example/Template Patterns.\n- **Confidence Scoring:** A helper function, `_calculate_ai_language_confidence()`, was created to calculate a confidence score (0-100) based on the density and variety of patterns found.\n- **Robust Extraction:** A helper function, `_extract_comments_and_docstrings()`, uses both line parsing and AST analysis for more accurate extraction.\n- **Testing:** A suite of 9 comprehensive test cases was created, covering all pattern types, human code, mixed content, and edge cases (e.g., empty input, no comments). All tests are passing.\n</info added on 2025-08-04T11:45:30.350Z>",
            "status": "done",
            "testStrategy": "Unit test with code containing known AI conversational phrases in various casings and code without them. Assert that the function correctly identifies the presence of these phrases."
          },
          {
            "id": 5,
            "title": "Integrate Heuristics in a Main Engine Orchestrator",
            "description": "Create a primary `analyze` function within the `engine.py` module that orchestrates the execution of all individual heuristic checks and aggregates their raw results into a single dictionary.",
            "dependencies": [
              "2.1",
              "2.2",
              "2.3",
              "2.4"
            ],
            "details": "In `engine.py`, define a main function `analyze(code_string)`. This function will call the four heuristic functions (`analyze_comment_patterns`, `analyze_variable_names`, etc.). It will aggregate the raw outputs into a structured dictionary, for example: `{'comment_patterns': {...}, 'variable_names': {...}}`. This dictionary will serve as the raw data for the confidence scoring logic (Task 8).\n<info added on 2025-08-04T12:03:41.596Z>\nThe `analyze()` function has been implemented as the main entry point, orchestrating all four heuristic functions. The implementation includes robust error handling for each heuristic to prevent a single failure from stopping the entire analysis. The final output is a structured dictionary containing the results from each heuristic, plus additional `summary` (with high-level statistics and a preliminary risk score) and `analysis_metadata` (timestamp, code length, errors) sections. A comprehensive suite of 8 integration tests was created and is passing, covering various code types and edge cases.\n</info added on 2025-08-04T12:03:41.596Z>",
            "status": "done",
            "testStrategy": "Create an integration test that passes a sample code snippet to the main `analyze` function. Mock the individual heuristic functions to return predictable results and assert that the final aggregated dictionary is structured correctly."
          }
        ]
      },
      {
        "id": 3,
        "title": "Implement Multi-Language File Parser",
        "description": "Build a module to handle various input methods, including single file uploads, raw text input, and batch file processing. This component will abstract the input source and provide clean code text to the detection engine.",
        "details": "Create a `parser.py` module. Implement a function that accepts a file path or a string. Use file extensions (`.py`, `.js`, `.java`, etc.) to identify the language. For now, treat all as plain text, but structure for future language-specific parsing. Implement logic to handle reading up to 5 files from a directory for batch analysis.",
        "testStrategy": "Test with various file types (.py, .js, .java, .cpp, .go, .rs) to ensure they are read correctly. Test the text input functionality. Test batch processing by providing a directory path and verifying all files are processed. Test edge cases like empty files or files > 5MB.",
        "priority": "high",
        "dependencies": [
          1
        ],
        "status": "done",
        "subtasks": [
          {
            "id": 1,
            "title": "Implement Core File and String Reader with Encoding Detection",
            "description": "Create the foundational function in `parser.py` that reads content from a single file path or a raw string. This function must robustly handle different text encodings to prevent errors and ensure accurate content retrieval.",
            "dependencies": [],
            "details": "Implement a function `read_source(source: Union[str, Path]) -> Tuple[str, str]`. It will use `pathlib.Path` to check if the source is a file. If it is, read the file's content, attempting to decode with UTF-8 first and falling back to a common encoding like 'latin-1' upon `UnicodeDecodeError`. If the source is not a file path, treat it as raw string input. The function should return the content and a source identifier (filepath or 'raw_string').\n<info added on 2025-08-04T12:35:48.444Z>\nThe core `read_source_with_language` function from the previous subtask now successfully reads content from files and raw strings, handles basic encoding issues (UTF-8/Latin-1 fallback), and identifies the source language. It also includes foundational error handling for file access and encoding problems.\n\nThis subtask will build a more robust validation layer on top of this foundation to handle a wider range of invalid inputs and edge cases gracefully. The goal is to prevent downstream errors and provide clear feedback to the user.\n\n**Implementation Plan:**\n- Add an explicit check to reject directory paths passed as input, raising a specific `IsADirectoryError` or a custom exception.\n- Implement a file size limit (e.g., 5MB, configurable) to avoid processing excessively large files. Raise an error if a file exceeds this limit.\n- Enhance file permission error handling to catch `PermissionError` and return a user-friendly message.\n- Validate that input strings are not excessively long or empty, establishing sensible constraints for raw text analysis.\n</info added on 2025-08-04T12:35:48.444Z>",
            "status": "done",
            "testStrategy": "Test with a file path containing UTF-8 characters. Test with a raw string input. Test with a file saved in a non-UTF-8 encoding (e.g., Latin-1) to ensure it is read correctly. Test with an empty file."
          },
          {
            "id": 2,
            "title": "Develop Input Validation and Error Handling Logic",
            "description": "Enhance the parser with robust validation to handle invalid inputs gracefully. This includes checking for path existence, file permissions, and file size limits to ensure the module is resilient.",
            "dependencies": [
              "3.1"
            ],
            "details": "Augment the `read_source` function to perform checks before reading. Use `pathlib.Path.exists()`, `pathlib.Path.is_file()`, and `os.access()` for read permissions. Implement a check to ensure file size does not exceed a 5MB limit. Raise specific, clear exceptions for each failure case (e.g., `FileNotFoundError`, `PermissionError`, `FileTooLargeError`).\n<info added on 2025-08-04T12:53:06.385Z>\n**Implementation Summary:**\n- A custom exception hierarchy was created (`ParserError`, `FileTooLargeError`, `InvalidInputError`) for more specific error handling.\n- Comprehensive validation was added for file inputs, checking for existence, permissions, file type, and the 5MB size limit.\n- New validation logic was implemented for string inputs, including a 1M character limit and null byte detection.\n- The system now correctly identifies and rejects directory paths, raising `IsADirectoryError`.\n- The `read_source` function was enhanced to integrate all validation checks and propagate errors correctly.\n- **Testing:** The implementation is fully tested, with 38/38 passing tests and 94% code coverage for the `parser.py` module.\n</info added on 2025-08-04T12:53:06.385Z>",
            "status": "done",
            "testStrategy": "Provide a path to a non-existent file and assert `FileNotFoundError`. Test with a file that lacks read permissions. Test with a file larger than 5MB to verify that `FileTooLargeError` is raised. Test with a directory path to ensure it's identified as an invalid input for this specific function."
          },
          {
            "id": 3,
            "title": "Implement Language Identification via File Extension",
            "description": "Add a mechanism to identify the programming language of a file based on its extension. This provides metadata for the detection engine and allows for future language-specific parsing.",
            "dependencies": [
              "3.1"
            ],
            "details": "Create a class or dictionary that maps common code file extensions (e.g., '.py', '.js', '.java', '.cpp', '.go', '.rs') to language names ('Python', 'JavaScript', etc.). Modify the `read_source` function to use `pathlib.Path.suffix` to look up the language. It should return the content, language, and source path. For raw strings or unknown extensions, default the language to 'plaintext'.",
            "status": "done",
            "testStrategy": "Test with various file paths like 'app.py', 'server.go', and 'component.js' to assert the correct language is returned. Test with a file having an unmapped extension (e.g., 'notes.txt') to ensure it defaults to 'plaintext'. Verify that raw string input also results in 'plaintext'."
          },
          {
            "id": 4,
            "title": "Implement Batch Processing for Directories",
            "description": "Develop the functionality to process multiple files from a directory. The function should scan a given directory, filter for relevant source code files, and process a limited number of them.",
            "dependencies": [
              "3.2",
              "3.3"
            ],
            "details": "Create a new function `parse_directory(dir_path: Union[str, Path])`. This function will use `pathlib.Path.iterdir()` to scan the directory. It will use the language extension map (from subtask 3.3) to identify valid code files. It will then call the validated `read_source` function (from subtask 3.2) for each valid file, up to a maximum of 5 files. It should return a list of results, where each result contains the content, language, and file path.\n<info added on 2025-08-04T13:01:42.687Z>\nThe `parse_directory` function has been implemented with several enhancements beyond the initial scope. It now supports a configurable `max_files` limit (defaulting to 5), processes files in a deterministic alphabetical order for reproducibility, and gracefully skips individual files that cause errors (e.g., encoding, permissions) to ensure the batch process completes.\n\nA new, complementary function, `get_directory_stats`, has also been developed. This function provides a fast, performance-optimized analysis of a directory without reading file contents. It returns comprehensive statistics, including total file count, code file count, and a language-by-language breakdown of the files present.\n\nThe implementation is confirmed to be robust and production-ready, with 49/49 tests passing, achieving 93% code coverage, and including comprehensive logging and cross-platform support.\n</info added on 2025-08-04T13:01:42.687Z>",
            "status": "done",
            "testStrategy": "Create a test directory with 7 code files and 2 non-code files. Call the function and verify that it processes exactly 5 code files. Test with a directory containing 3 code files and ensure all 3 are processed. Test with an empty directory to confirm it returns an empty list."
          },
          {
            "id": 5,
            "title": "Create Unified Parser Interface for Engine Integration",
            "description": "Abstract the different input methods (single file, raw string, directory) into a single, high-level interface function. This provides a clean and consistent entry point for the detection engine.",
            "dependencies": [
              "3.4"
            ],
            "details": "Implement a primary `parse(source: Union[str, Path]) -> List[Dict]` function in `parser.py`. This function will determine the input type: if `pathlib.Path(source).is_dir()`, it calls `parse_directory`; if `pathlib.Path(source).is_file()`, it calls `read_source`; otherwise, it treats the input as a raw string. The function must always return a list of dictionaries, each containing `{'content': str, 'language': str, 'source': str}`, to provide a standardized output for the detection engine.\n<info added on 2025-08-04T13:22:45.835Z>\nThe primary `parse()` function was implemented as specified. In addition, an enhanced `parse_with_stats()` function was created to return detailed processing statistics, including performance timing, file counts, and language distribution. The output schema for both functions was enriched to include a `metadata` key containing contextual details like file size and input type, providing a more robust data structure for the detection engine. The implementation is validated by a comprehensive test suite (92% code coverage) and is ready for integration.\n</info added on 2025-08-04T13:22:45.835Z>",
            "status": "done",
            "testStrategy": "Test the `parse` function by passing it a valid file path, a directory path, and a raw code string. In each case, assert that the output is a list of dictionaries with the expected structure and content. Verify that it correctly delegates to the appropriate internal function."
          }
        ]
      },
      {
        "id": 4,
        "title": "Build CLI Interface",
        "description": "Create a command-line interface (CLI) for the tool, allowing users to run analysis directly from their terminal. This provides a fast and scriptable way to use the detection engine.",
        "details": "Use Python's `argparse` or a more modern library like `click`. Implement the specified commands: `shadow-detect analyze <file>`, `shadow-detect check --text \"...\"`, and `shadow-detect scan <directory> --recursive`. Structure the CLI to call the appropriate functions from the parser and engine modules.",
        "testStrategy": "Run each command from the terminal with valid and invalid arguments. Verify that `analyze` works with a single file, `check` with a string, and `scan` with a directory. Check that help messages (`-h` or `--help`) are displayed correctly.",
        "priority": "high",
        "dependencies": [
          1
        ],
        "status": "done",
        "subtasks": []
      },
      {
        "id": 5,
        "title": "Integrate Engine with CLI and Format Output",
        "description": "Connect the detection engine to the CLI and format the analysis results into a user-friendly text output, including the final verdict, confidence score, and reasons.",
        "details": "In the CLI module, after parsing and analysis, call a formatting function. This function will take the raw results from the engine (e.g., a dictionary of triggered heuristics) and generate the final output string: 'Result: Likely AI-Generated', 'Confidence: 75%', 'Reason: ...', 'Patterns Found: [Generic variable names, Repetitive comments]'.",
        "testStrategy": "Run the CLI tool on test files. Verify that the output matches the expected format. Check that the confidence score and reasons are plausible based on the input code. This is an integration test.",
        "priority": "high",
        "dependencies": [
          2,
          3,
          4
        ],
        "status": "done",
        "subtasks": []
      },
      {
        "id": 6,
        "title": "Develop Backend API for Web Interface",
        "description": "Set up a lightweight web server using FastAPI or Flask to expose the detection engine's functionality via API endpoints. This will serve as the backend for the web UI.",
        "details": "Use FastAPI for its performance and automatic docs. Create a `main.py` for the web server. Define two endpoints: one for text input (`/api/check`) that accepts a JSON payload with code, and one for file upload (`/api/analyze`) that accepts a file. These endpoints will use the engine and return results as JSON.",
        "testStrategy": "Run the web server locally. Use a tool like `curl` or Postman to send requests to the `/api/check` and `/api/analyze` endpoints. Verify that they return a 200 status code and a valid JSON response containing the analysis results.",
        "priority": "medium",
        "dependencies": [
          2,
          3
        ],
        "status": "done",
        "subtasks": []
      },
      {
        "id": 7,
        "title": "Create Simple Web Frontend",
        "description": "Build a minimal HTML/CSS/JavaScript frontend for users to interact with the tool through a web browser. The interface will support pasting code and uploading files.",
        "details": "Create a `/static` directory for the frontend files. Write a single `index.html` file with a text area for code input and a drag-and-drop file upload zone. Use vanilla JavaScript to handle form submission, sending the data to the backend API using `fetch()`. On response, dynamically display the results (verdict, confidence, explanation) on the page.",
        "testStrategy": "Open the `index.html` in a browser. Test pasting code and clicking 'Analyze'. Test dragging and dropping a file. Verify that the results from the backend are displayed correctly and clearly on the page. Check for basic responsiveness.",
        "priority": "medium",
        "dependencies": [
          6
        ],
        "status": "done",
        "subtasks": []
      },
      {
        "id": 8,
        "title": "Implement Confidence Scoring Logic",
        "description": "Develop the algorithm to convert the raw heuristic detection results into a quantitative confidence score (0-100%) and a qualitative risk level (Low, Medium, High).",
        "details": "In the detection engine, create a scoring module. Assign a weight to each heuristic. The final score will be a weighted sum of the triggered heuristics, normalized to a 0-100 scale. Define thresholds for risk levels (e.g., 0-40% Low, 41-75% Medium, 76-100% High). The score should be part of the engine's output.",
        "testStrategy": "Create unit tests that provide mock heuristic results to the scoring function. Assert that a file with many triggered heuristics gets a high score, and a file with few gets a low score. Verify that the risk levels correspond correctly to the score thresholds.",
        "priority": "medium",
        "dependencies": [
          2
        ],
        "status": "done",
        "subtasks": []
      },
      {
        "id": 9,
        "title": "Develop Interactive AI Pattern Quiz",
        "description": "Create the 'Interactive AI Pattern Quiz' feature to educate users. This involves creating a set of quiz questions and a simple UI to present them.",
        "details": "Create a JSON file with 10 quiz questions, each containing a code snippet, the correct answer ('AI' or 'Human'), and the tool's reasoning. Create a new API endpoint (`/api/quiz`) in the backend to serve these questions. On the frontend, create a `quiz.html` page that fetches the questions, allows the user to guess, and then shows the correct answer and explanation.",
        "testStrategy": "Manually take the quiz to ensure the flow is correct. Verify that questions are loaded from the backend. Check that user answers are recorded and the correct results and explanations are shown. Ensure the user's score is tracked correctly.",
        "priority": "medium",
        "dependencies": [
          6,
          7
        ],
        "status": "done",
        "subtasks": []
      },
      {
        "id": 10,
        "title": "Implement Code Style Inconsistency Detector",
        "description": "Add an advanced detection feature to identify when coding style (e.g., indentation, spacing, naming conventions) changes within a single file, which can indicate AI-injected code. This feature is now complete and integrated into the main analysis engine.",
        "status": "done",
        "dependencies": [
          2
        ],
        "priority": "medium",
        "details": "Implemented a core function `analyze_style_inconsistency()` that creates style fingerprints for each function and class. The analysis includes: indentation style (tabs vs. spaces), naming conventions (snake_case, camelCase, PascalCase), comment density, and line length variance. The detector is fully integrated into the main `analyze()` function in `engine.py`, the scoring system in `scoring.py`, and the CLI output.",
        "testStrategy": "Created a comprehensive test suite `test_style_inconsistency.py` with 11 passing test cases covering consistent code, mixed indentation, mixed naming conventions, comment density differences, and line length patterns. End-to-end verification was performed using the CLI on a test file with mixed styles, confirming correct detection and integration with the confidence score.",
        "subtasks": [
          {
            "id": 1,
            "title": "Develop style fingerprinting function",
            "description": "Create the `analyze_style_inconsistency` function to analyze and fingerprint indentation, naming, comment density, and line length.",
            "status": "done",
            "dependencies": [],
            "details": "",
            "testStrategy": ""
          },
          {
            "id": 2,
            "title": "Integrate detector into analysis engine",
            "description": "Add the style inconsistency check to the main `analyze()` function in `engine.py`.",
            "status": "done",
            "dependencies": [],
            "details": "",
            "testStrategy": ""
          },
          {
            "id": 3,
            "title": "Update scoring system",
            "description": "Add appropriate weights for style inconsistency findings to the scoring logic in `scoring.py`.",
            "status": "done",
            "dependencies": [],
            "details": "",
            "testStrategy": ""
          },
          {
            "id": 4,
            "title": "Create unit tests",
            "description": "Implement 11 unit tests in `test_style_inconsistency.py` to cover all detection scenarios and edge cases.",
            "status": "done",
            "dependencies": [],
            "details": "",
            "testStrategy": ""
          },
          {
            "id": 5,
            "title": "Perform end-to-end CLI testing",
            "description": "Verify the full feature from detection to scoring and output using the CLI and real-world test files.",
            "status": "done",
            "dependencies": [],
            "details": "",
            "testStrategy": ""
          }
        ]
      },
      {
        "id": 11,
        "title": "Set Up SQLite Database and History Dashboard",
        "description": "Implement a simple persistence layer using SQLite to store analysis history. Create a basic dashboard page to display past results and simple statistics.",
        "details": "Use Python's built-in `sqlite3` module. Create a database schema with a 'history' table (id, filename, timestamp, result, score). Modify the backend API to log every analysis to this table. Create a new `/dashboard` page and a corresponding API endpoint (`/api/history`) that retrieves all records. Display the history in a simple HTML table on the dashboard page.",
        "testStrategy": "Perform several analyses via the web UI. Navigate to the dashboard page and verify that all analyses are listed correctly. Check the database file directly to ensure data is being written as expected. Restart the server and confirm the history persists.",
        "priority": "low",
        "dependencies": [
          6,
          7
        ],
        "status": "done",
        "subtasks": [
          {
            "id": 1,
            "title": "Define and Create SQLite Database Schema",
            "description": "Implement a Python module to initialize the SQLite database file and create the 'history' table with the specified schema if it does not already exist.",
            "dependencies": [],
            "details": "Using Python's `sqlite3` module, create a function that executes a `CREATE TABLE IF NOT EXISTS history (id INTEGER PRIMARY KEY AUTOINCREMENT, filename TEXT NOT NULL, timestamp TEXT NOT NULL, result TEXT NOT NULL, score INTEGER NOT NULL);`. This function should be called on application startup.",
            "status": "done",
            "testStrategy": "Run the application startup sequence. Verify that a database file (e.g., `history.db`) is created. Use a database inspection tool to confirm the 'history' table exists with the correct columns and data types."
          },
          {
            "id": 2,
            "title": "Integrate Analysis Logging into Backend API",
            "description": "Modify the existing backend analysis endpoint to save the results of each analysis—filename, timestamp, result, and score—into the 'history' table.",
            "dependencies": [
              "11.1"
            ],
            "details": "After an analysis is completed within the main API logic, gather the results. Connect to the SQLite database and execute an `INSERT` statement to add the new record. Ensure the timestamp is stored in a consistent format like ISO 8601.",
            "status": "done",
            "testStrategy": "Perform several analyses via the web UI. Query the `history` table directly to verify that a new row has been added for each analysis with the correct filename, timestamp, result, and score."
          },
          {
            "id": 3,
            "title": "Create `/api/history` Endpoint for Data Retrieval",
            "description": "Implement a new GET endpoint at `/api/history` that queries the SQLite database, retrieves all records from the 'history' table, and returns them as a JSON response.",
            "dependencies": [
              "11.1"
            ],
            "details": "Add a new route to the backend web framework. The handler for this route will execute a `SELECT * FROM history ORDER BY timestamp DESC` query. The fetched rows should be converted into a list of dictionaries and returned as a JSON array.",
            "status": "done",
            "testStrategy": "After logging several analyses, access the `/api/history` endpoint using a browser or API client like Postman. Verify the response is a 200 OK with a valid JSON array containing all the logged records."
          },
          {
            "id": 4,
            "title": "Create Basic HTML Structure for `/dashboard` Page",
            "description": "Create a new route `/dashboard` that serves a new `dashboard.html` template. This page will contain the basic layout and an empty table for displaying the history.",
            "dependencies": [],
            "details": "Create a `dashboard.html` file in the templates directory. The file should include a title and an HTML table with a `<thead>` containing headers for 'Filename', 'Timestamp', 'Result', and 'Score'. Create a corresponding backend route to render this template.",
            "status": "done",
            "testStrategy": "Start the web server and navigate to the `/dashboard` URL in a browser. Verify that the page loads correctly and displays the static elements, including the empty table with its headers."
          },
          {
            "id": 5,
            "title": "Populate Dashboard Table with Data from API",
            "description": "Add client-side JavaScript to the dashboard page to fetch data from the `/api/history` endpoint on page load and dynamically populate the HTML table.",
            "dependencies": [
              "11.3",
              "11.4"
            ],
            "details": "In a `<script>` tag on `dashboard.html`, use the `fetch` API to make a GET request to `/api/history`. Upon receiving the JSON response, iterate through the records and dynamically create and insert `<tr>` and `<td>` elements into the table body.",
            "status": "done",
            "testStrategy": "Load the `/dashboard` page in a browser. Open the developer tools to monitor network requests. Verify a successful request is made to `/api/history` and that the table on the page is populated with the data returned from the API."
          }
        ]
      },
      {
        "id": 12,
        "title": "Finalize Documentation and Test Suite",
        "description": "Create comprehensive user documentation and a robust test suite to ensure the tool is reliable, easy to use, and maintainable.",
        "details": "Write a detailed `README.md` file covering: project overview, installation instructions (for both CLI and web), usage examples for all CLI commands, and an explanation of the output format. Expand the test suite in `/tests` to have at least 80% code coverage, including integration tests for the CLI and API.",
        "testStrategy": "Ask a new user to follow the `README.md` to install and run the tool. Their ability to do so without assistance is the primary test. Run a code coverage tool (e.g., `pytest-cov`) to verify test coverage meets the target. All tests must pass.",
        "priority": "high",
        "dependencies": [
          5,
          7,
          8,
          9,
          10,
          11
        ],
        "status": "done",
        "subtasks": [
          {
            "id": 1,
            "title": "Draft Core README Sections (Overview & Output Format)",
            "description": "Create the `README.md` file and write the initial sections, including a compelling project overview and a clear explanation of the analysis output format. This sets the foundation for the rest of the documentation.",
            "dependencies": [],
            "details": "In the `README.md`, write a 'Project Overview' section explaining what the tool does and its purpose. Create an 'Output Format' section that details the meaning of 'Verdict', 'Confidence', and 'Patterns Found', referencing the format established in Task 5.",
            "status": "done",
            "testStrategy": "Review the written sections for clarity, accuracy, and completeness. Ensure the output format explanation matches the actual tool output."
          },
          {
            "id": 2,
            "title": "Document Installation and Usage Instructions in README",
            "description": "Add comprehensive installation and usage instructions to the `README.md` for both the Command-Line Interface (CLI) and the web frontend, ensuring a new user can get started without assistance.",
            "dependencies": [
              "12.1"
            ],
            "details": "Provide step-by-step instructions for cloning the repo, setting up the Python environment, and installing dependencies. Detail how to run the web server for the web UI. For the CLI, provide usage examples for every command, showing the command itself and its expected output.",
            "status": "done",
            "testStrategy": "Follow the instructions from a clean environment to verify they are correct. This is the primary test mentioned in the parent task's test strategy."
          },
          {
            "id": 3,
            "title": "Implement CLI Integration Tests",
            "description": "Develop a suite of integration tests for the Command-Line Interface to verify that it correctly processes various inputs (files, strings) and produces the expected output format and exit codes.",
            "dependencies": [],
            "details": "In the `/tests` directory, create a new test file for CLI integration. Use Python's `subprocess` module or a test framework's CLI runner to execute the main script with different arguments. Test with sample files containing AI-generated and human-written code. Assert that `stdout` matches the expected output format.",
            "status": "done",
            "testStrategy": "Run the tests and ensure they cover different command-line arguments and edge cases, such as non-existent files or invalid arguments."
          },
          {
            "id": 4,
            "title": "Expand API Test Coverage",
            "description": "Write unit and integration tests for all backend API endpoints to ensure they are robust and handle requests correctly. This includes the main analysis endpoint and the quiz endpoint.",
            "dependencies": [],
            "details": "Using a test client (e.g., from Flask or FastAPI), write tests for the `/api/analyze` and `/api/quiz` endpoints. For `/api/analyze`, test with valid code, empty input, and malformed requests. For `/api/quiz`, verify that the endpoint returns the correct JSON structure. Ensure all tests are added to the `/tests` directory.",
            "status": "done",
            "testStrategy": "Check that tests cover both successful (200 OK) and error (e.g., 400 Bad Request) responses from the API. Verify the content and structure of the JSON responses."
          },
          {
            "id": 5,
            "title": "Achieve 80% Code Coverage and Finalize",
            "description": "Run the complete test suite with a coverage tool, identify untested code paths, and add the necessary unit tests to meet the 80% coverage target. Perform a final validation of the documentation.",
            "dependencies": [
              "12.2",
              "12.3",
              "12.4"
            ],
            "details": "Use a tool like `pytest-cov` to generate a coverage report (`pytest --cov=shadow_ai`). Analyze the report to find modules or functions with low coverage. Write targeted unit tests for helper functions, edge cases in the detection engine, and any other logic not covered by integration tests. Finally, ask a new user to follow the `README.md` to confirm its effectiveness.",
            "status": "done",
            "testStrategy": "Verify that the `pytest-cov` command reports a total coverage of 80% or higher. Confirm that a new user can successfully install and run the tool using only the `README.md`."
          }
        ]
      }
    ],
    "metadata": {
      "created": "2025-08-04T09:46:46.647Z",
      "updated": "2025-08-04T19:20:34.651Z",
      "description": "Tasks for master context"
    }
  }
}